{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: games.csv | rows=140,082 cols=7\n",
      "── Files were read.   Time used: 0m  0.68s\n",
      "Saved: /Users/timtoepper/Downloads/Banks_Assignment_Tim/sampled_outputs/games_cleaned.csv | rows=140,082 cols=14\n",
      "Saved: /Users/timtoepper/Downloads/Banks_Assignment_Tim/sampled_outputs/games_sampled.csv | rows=10,000 cols=14\n",
      "── Processing completed.   Time used: 0m  10.72s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import time as t\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "\n",
    "# ====== Settings (edit these two as needed) ======\n",
    "input_dir = Path(\"/Users/timtoepper/Downloads/Banks_Assignment_Tim\")   # folder with CSVs\n",
    "output_dir = input_dir / \"sampled_outputs\"                    # where to save sampled files\n",
    "# =================================================\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# For simple time measurements in logs\n",
    "start = t.time()\n",
    "\n",
    "# Sampling parameters\n",
    "TOTAL_RECORDS = 10_000\n",
    "FIXED_RECORDS = 1_000\n",
    "BLOCK_SIZE = 100\n",
    "NUM_RANDOM_BLOCKS = (TOTAL_RECORDS - FIXED_RECORDS) // BLOCK_SIZE  # 90 blocks\n",
    "\n",
    "RNG = np.random.default_rng(42)  # reproducible sampling\n",
    "\n",
    "# ────── Processing Functions ────────────────────────────────────────────────────\n",
    "def parse_price(j: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Parse a JSON string from price_overview; return normalized fields.\"\"\"\n",
    "    if pd.isna(j):\n",
    "        return {\n",
    "            \"price_final_cents\": None,\n",
    "            \"price_initial_cents\": None,\n",
    "            \"currency\": None,\n",
    "            \"discount_percent\": None,\n",
    "            \"price_final\": None,\n",
    "            \"price_initial\": None,\n",
    "            \"price_final_formatted\": None,\n",
    "        }\n",
    "    try:\n",
    "        obj = json.loads(j)\n",
    "        return {\n",
    "            \"price_final_cents\": obj.get(\"final\"),\n",
    "            \"price_initial_cents\": obj.get(\"initial\"),\n",
    "            \"currency\": obj.get(\"currency\"),\n",
    "            \"discount_percent\": obj.get(\"discount_percent\"),\n",
    "            \"price_final\": (obj.get(\"final\") or 0) / 100.0,\n",
    "            \"price_initial\": (obj.get(\"initial\") or 0) / 100.0,\n",
    "            \"price_final_formatted\": obj.get(\"final_formatted\"),\n",
    "        }\n",
    "    except Exception:\n",
    "        # Fallback: return empty fields if a row is malformed\n",
    "        return {\n",
    "            \"price_final_cents\": None,\n",
    "            \"price_initial_cents\": None,\n",
    "            \"currency\": None,\n",
    "            \"discount_percent\": None,\n",
    "            \"price_final\": None,\n",
    "            \"price_initial\": None,\n",
    "            \"price_final_formatted\": None,\n",
    "        }\n",
    "\n",
    "def clean_languages(raw: Any, TAG_RE: re.Pattern) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Remove HTML tags, split by comma, normalize names, and detect languages\n",
    "    with full audio support (marked by a trailing '*').\n",
    "    Returns (all_languages, full_audio_subset).\n",
    "    \"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return [], []\n",
    "    # Remove <br> and other tags, keep the \"*\" that indicates full audio support\n",
    "    txt = TAG_RE.sub(\"\", str(raw))\n",
    "    # Now split on commas and normalize\n",
    "    langs = [p.strip() for p in txt.split(\",\") if p.strip()]\n",
    "    # Items may look like \"English*\" where * = full audio support\n",
    "    full_audio = [l.replace(\"*\", \"\").strip() for l in langs if l.endswith(\"*\")]\n",
    "    normal = [l.replace(\"*\", \"\").strip() for l in langs]\n",
    "    return normal, full_audio\n",
    "\n",
    "def time_helper(text: str) -> None:\n",
    "    print(f\"── {text}   Time used: {int((t.time()-start)//60)}m  {round((t.time()-start)%60, 2)}s\")\n",
    "\n",
    "def read_games_csv(input_dir: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Try to read a games CSV from common filenames; return None if not found.\"\"\"\n",
    "    candidates = [\n",
    "        input_dir / \"games_cleaned.csv\",\n",
    "        input_dir / \"steam_games.csv\",\n",
    "        input_dir / \"Games_cleaned.csv\",\n",
    "    ]\n",
    "    if not any(p.exists() for p in candidates):\n",
    "        # try globbing as a fallback\n",
    "        globs = list(input_dir.glob(\"games*.csv\")) + list(input_dir.glob(\"*games*.csv\"))\n",
    "        candidates.extend(globs)\n",
    "\n",
    "    for p in candidates:\n",
    "        if isinstance(p, Path) and p.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    p,\n",
    "                    engine=\"python\",\n",
    "                    sep=\",\",\n",
    "                    quotechar='\"',\n",
    "                    escapechar=\"\\\\\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                )\n",
    "                print(f\"Loaded: {p.name} | rows={len(df):,} cols={len(df.columns)}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to read {p}: {e}\")\n",
    "    return None\n",
    "\n",
    "def expand_price_overview(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"If 'price_overview' exists, expand its JSON into columns.\"\"\"\n",
    "    if \"price_overview\" not in df.columns:\n",
    "        print(\"Note: 'price_overview' column not found; skipping JSON expansion.\")\n",
    "        return df\n",
    "    price_expanded = df[\"price_overview\"].apply(parse_price).apply(pd.Series)\n",
    "    # Avoid duplicate columns if some already exist with same names\n",
    "    price_expanded = price_expanded[[c for c in price_expanded.columns if c not in df.columns]]\n",
    "    df_out = pd.concat([df.drop(columns=[\"price_overview\"]), price_expanded], axis=1)\n",
    "    return df_out\n",
    "\n",
    "def expand_languages(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"If 'languages' exists, clean it and add two list-typed columns.\"\"\"\n",
    "    if \"languages\" not in df.columns:\n",
    "        print(\"Note: 'languages' column not found; skipping language cleaning.\")\n",
    "        return df\n",
    "    TAG_RE = re.compile(r\"<.*?>\")\n",
    "    langs_parsed = df[\"languages\"].apply(lambda x: clean_languages(x, TAG_RE))\n",
    "    df[\"languages_clean\"] = langs_parsed.apply(lambda t: t[0]).astype(object)\n",
    "    df[\"languages_full_audio\"] = langs_parsed.apply(lambda t: t[1]).astype(object)\n",
    "    # keep or drop original column depending on your preference\n",
    "    df = df.drop(columns=[\"languages\"])\n",
    "    return df\n",
    "\n",
    "def to_csv(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Write DataFrame to CSV with safe options for large text.\"\"\"\n",
    "    df.to_csv(path, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    print(f\"Saved: {path} | rows={len(df):,} cols={len(df.columns)}\")\n",
    "\n",
    "def sample_blocks(\n",
    "    df: pd.DataFrame,\n",
    "    total_records: int,\n",
    "    fixed_records: int,\n",
    "    block_size: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Take first `fixed_records` rows (if available), then sample N random blocks of size `block_size`\n",
    "    from the remaining rows without overlap. Adjusts gracefully if the dataset is smaller.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return df\n",
    "\n",
    "    # If not enough rows for the target total, just return the entire df\n",
    "    if n <= total_records:\n",
    "        print(f\"Sampling note: dataset has only {n:,} rows <= {total_records:,}; returning full dataset.\")\n",
    "        return df.copy()\n",
    "\n",
    "    # Fixed head\n",
    "    fixed_end = min(fixed_records, n)\n",
    "    fixed_part = df.iloc[:fixed_end]\n",
    "\n",
    "    remaining = df.iloc[fixed_end:].reset_index(drop=True)\n",
    "    remaining_n = len(remaining)\n",
    "\n",
    "    # How many blocks can we realistically draw?\n",
    "    max_blocks = remaining_n // block_size\n",
    "    want_blocks = max((total_records - fixed_end) // block_size, 0)\n",
    "    draw_blocks = min(max_blocks, want_blocks)\n",
    "\n",
    "    # Choose block starting indices without overlap (step = block_size)\n",
    "    possible_starts = np.arange(0, remaining_n - block_size + 1, block_size)\n",
    "    if len(possible_starts) == 0 or draw_blocks == 0:\n",
    "        sampled_part = remaining\n",
    "    else:\n",
    "        chosen_idx = rng.choice(possible_starts, size=draw_blocks, replace=False)\n",
    "        blocks = [remaining.iloc[s : s + block_size] for s in chosen_idx]\n",
    "        sampled_part = pd.concat(blocks, axis=0).sort_index()\n",
    "\n",
    "    out = pd.concat([fixed_part, sampled_part], axis=0)\n",
    "    # If we overshot (unlikely), trim\n",
    "    if len(out) > total_records:\n",
    "        out = out.iloc[:total_records]\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "# ────── Main pipeline ───────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) Read games CSV\n",
    "    df_games = read_games_csv(input_dir)\n",
    "    if df_games is None:\n",
    "        print(\"ERROR: Could not find a games CSV in the input directory. \"\n",
    "              \"Expected something like 'games_cleaned.csv'.\")\n",
    "        return\n",
    "\n",
    "    time_helper(\"Files were read.\")\n",
    "\n",
    "    # 2) Expand price_overview JSON and clean languages (if present)\n",
    "    df_games = expand_price_overview(df_games)\n",
    "    df_games = expand_languages(df_games)\n",
    "\n",
    "    # 3) Save cleaned data\n",
    "    cleaned_path = output_dir / \"games_cleaned.csv\"\n",
    "    to_csv(df_games, cleaned_path)\n",
    "\n",
    "    # 4) Optional sampling (only meaningful when df is large)\n",
    "    sampled_df = sample_blocks(\n",
    "        df_games,\n",
    "        total_records=TOTAL_RECORDS,\n",
    "        fixed_records=FIXED_RECORDS,\n",
    "        block_size=BLOCK_SIZE,\n",
    "        rng=RNG,\n",
    "    )\n",
    "    sampled_path = output_dir / \"games_sampled.csv\"\n",
    "    to_csv(sampled_df, sampled_path)\n",
    "\n",
    "    time_helper(\"Processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7211f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5f84e3",
   "metadata": {},
   "source": [
    "PReprocessing merger final + Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a302ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Script 6 outputs - using cleaned games data\n",
      "Data directory: /Users/timtoepper/Downloads/Banks_Assignment_Tim\n",
      "Games file: /Users/timtoepper/Downloads/Banks_Assignment_Tim/sampled_outputs/games_cleaned.csv\n",
      "================================================================================\n",
      "\n",
      "1. LOADING AND CLEANING GAMES.CSV\n",
      "------------------------------------------------------------\n",
      "Loaded: 140,082 games from games_cleaned.csv\n",
      "Cleaned games: 140,082 rows × 17 columns\n",
      "\n",
      "2. LOADING AND CLEANING STEAMSPY_INSIGHTS.CSV\n",
      "------------------------------------------------------------\n",
      "  Processing row 25,000...\n",
      "  Processing row 50,000...\n",
      "  Processing row 75,000...\n",
      "  Processing row 100,000...\n",
      "  Processing row 125,000...\n",
      "Loaded: 140,077 records\n",
      "  Parsed owners - Average: 72,581\n",
      "Merged steamspy: 140,082 rows × 34 columns\n",
      "\n",
      "3. LOADING AND CLEANING REVIEWS.CSV\n",
      "------------------------------------------------------------\n",
      "Loaded: 140,086 records\n",
      "  Cleaning reviews text...\n",
      "  Cleaned reviews text: 11,698 valid reviews\n",
      "Merged reviews: 140,082 rows × 49 columns\n",
      "\n",
      "4. LOADING AND CLEANING CATEGORIES.CSV\n",
      "------------------------------------------------------------\n",
      "Loaded: 522,582 records\n",
      "Merged categories: 140,082 rows × 56 columns\n",
      "\n",
      "5. LOADING AND CLEANING TAGS.CSV (ALL TAGS)\n",
      "------------------------------------------------------------\n",
      "Loaded: 1,744,632 records\n",
      "Merged tags: 140,082 rows × 65 columns\n",
      "  Average tags per game: 14.8\n",
      "  Max tags on a game: 20\n",
      "\n",
      "6. LOADING AND CLEANING GENRES.CSV\n",
      "------------------------------------------------------------\n",
      "Loaded: 353,339 records\n",
      "Merged genres: 140,082 rows × 68 columns\n",
      "\n",
      "7. LOADING AND CLEANING DESCRIPTIONS.CSV (FULL DATASET)\n",
      "------------------------------------------------------------\n",
      "Loaded: 59,858 records\n",
      "  Cleaned summary: 40,616 valid descriptions\n",
      "  Cleaned extensive: 40,351 valid descriptions\n",
      "  Cleaned about: 40,347 valid descriptions\n",
      "Merged descriptions: 140,082 rows × 74 columns\n",
      "\n",
      "8. LOADING AND CLEANING PROMOTIONAL.CSV (FULL DATASET)\n",
      "------------------------------------------------------------\n",
      "Loaded: 10,558 records\n",
      "Merged promotional: 140,082 rows × 75 columns\n",
      "\n",
      "9. CALCULATING KEY PERFORMANCE INDICATORS (USD-BASED)\n",
      "------------------------------------------------------------\n",
      "Estimated total revenue (USD): $229,581,004,175\n",
      "Average game age: 3.5 years\n",
      "Successful games (>75th percentile): 34,915\n",
      "Average quality score: 44.3/100\n",
      "Hit games (>90th percentile): 13,126\n",
      "Valid publishers: 90,368 games\n",
      "\n",
      "CALCULATING ADVANCED M&A METRICS\n",
      "------------------------------------------------------------\n",
      "Calculating Content Production Metrics...\n",
      "Calculating Retention Metrics...\n",
      "Detecting Franchises...\n",
      "Calculating Audience Metrics...\n",
      "Calculating Financial Efficiency...\n",
      "Calculating Risk Metrics...\n",
      "Calculating Advanced Success Metrics...\n",
      "Commercial successes: 32,456\n",
      "Critical successes: 17,526\n",
      "Hidden gems: 293\n",
      "Dead games: 89,814\n",
      "\n",
      "CALCULATING PUBLISHER METRICS FOR M&A\n",
      "------------------------------------------------------------\n",
      "Saved publisher metrics: /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/publisher_metrics_cleaned.csv\n",
      "Saved M&A analysis: /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/publisher_ma_analysis.csv\n",
      "\n",
      "TOP 20 PUBLISHERS BY REVENUE (USD):\n",
      "================================================================================\n",
      "\n",
      "Bandai Namco Entertainment                        \n",
      "  Games: 24 | Revenue (USD): $81,528,458,056\n",
      "  Success Rate: 100.0% | Avg Rating: 82.0%\n",
      "  Avg Revenue/Game: $3,397,019,086\n",
      "\n",
      "Amazon Games                                      \n",
      "  Games: 2 | Revenue (USD): $4,904,182,500\n",
      "  Success Rate: 100.0% | Avg Rating: 70.0%\n",
      "  Avg Revenue/Game: $2,452,091,250\n",
      "\n",
      "Electronic Arts                                   \n",
      "  Games: 104 | Revenue (USD): $4,900,845,183\n",
      "  Success Rate: 94.2% | Avg Rating: 77.0%\n",
      "  Avg Revenue/Game: $47,123,511\n",
      "\n",
      "Ubisoft                                           \n",
      "  Games: 151 | Revenue (USD): $4,012,411,160\n",
      "  Success Rate: 90.1% | Avg Rating: 76.0%\n",
      "  Avg Revenue/Game: $26,572,259\n",
      "\n",
      "Bethesda Softworks                                \n",
      "  Games: 66 | Revenue (USD): $3,201,311,486\n",
      "  Success Rate: 93.9% | Avg Rating: 78.0%\n",
      "  Avg Revenue/Game: $48,504,719\n",
      "\n",
      "Xbox Game Studios                                 \n",
      "  Games: 57 | Revenue (USD): $3,175,929,441\n",
      "  Success Rate: 96.5% | Avg Rating: 84.0%\n",
      "  Avg Revenue/Game: $55,718,060\n",
      "\n",
      "PlayStation Publishing LLC                        \n",
      "  Games: 18 | Revenue (USD): $3,019,713,272\n",
      "  Success Rate: 88.9% | Avg Rating: 85.0%\n",
      "  Avg Revenue/Game: $167,761,848\n",
      "\n",
      "CAPCOM Co., Ltd.                                  \n",
      "  Games: 41 | Revenue (USD): $2,939,042,042\n",
      "  Success Rate: 97.6% | Avg Rating: 83.0%\n",
      "  Avg Revenue/Game: $71,683,952\n",
      "\n",
      "Game Science                                      \n",
      "  Games: 2 | Revenue (USD): $2,905,500,000\n",
      "  Success Rate: 100.0% | Avg Rating: 88.0%\n",
      "  Avg Revenue/Game: $1,452,750,000\n",
      "\n",
      "CD PROJEKT RED                                    \n",
      "  Games: 7 | Revenue (USD): $2,757,938,560\n",
      "  Success Rate: 100.0% | Avg Rating: 81.0%\n",
      "  Avg Revenue/Game: $393,991,223\n",
      "\n",
      "Valve                                             \n",
      "  Games: 36 | Revenue (USD): $2,576,703,550\n",
      "  Success Rate: 100.0% | Avg Rating: 89.0%\n",
      "  Avg Revenue/Game: $71,575,099\n",
      "\n",
      "Larian Studios                                    \n",
      "  Games: 7 | Revenue (USD): $2,554,066,095\n",
      "  Success Rate: 100.0% | Avg Rating: 84.0%\n",
      "  Avg Revenue/Game: $364,866,585\n",
      "\n",
      "Worldwalker Games LLC, WhisperGames               \n",
      "  Games: 1 | Revenue (USD): $2,475,000,000\n",
      "  Success Rate: 100.0% | Avg Rating: 95.0%\n",
      "  Avg Revenue/Game: $2,475,000,000\n",
      "\n",
      "FromSoftware, Inc., Bandai Namco Entertainment    \n",
      "  Games: 3 | Revenue (USD): $2,427,115,000\n",
      "  Success Rate: 100.0% | Avg Rating: 93.0%\n",
      "  Avg Revenue/Game: $809,038,333\n",
      "\n",
      "Pocketpair                                        \n",
      "  Games: 4 | Revenue (USD): $2,416,040,318\n",
      "  Success Rate: 100.0% | Avg Rating: 79.0%\n",
      "  Avg Revenue/Game: $604,010,079\n",
      "\n",
      "SEGA                                              \n",
      "  Games: 197 | Revenue (USD): $2,365,207,960\n",
      "  Success Rate: 65.0% | Avg Rating: 82.0%\n",
      "  Avg Revenue/Game: $12,006,132\n",
      "\n",
      "Paradox Interactive                               \n",
      "  Games: 85 | Revenue (USD): $2,324,587,822\n",
      "  Success Rate: 85.9% | Avg Rating: 73.0%\n",
      "  Avg Revenue/Game: $27,348,092\n",
      "\n",
      "Activision                                        \n",
      "  Games: 64 | Revenue (USD): $1,715,430,638\n",
      "  Success Rate: 92.2% | Avg Rating: 77.0%\n",
      "  Avg Revenue/Game: $26,803,604\n",
      "\n",
      "2K                                                \n",
      "  Games: 74 | Revenue (USD): $1,691,778,208\n",
      "  Success Rate: 87.8% | Avg Rating: 74.0%\n",
      "  Avg Revenue/Game: $22,861,868\n",
      "\n",
      "SEGA, Feral Interactive (Mac), Feral Interactive (\n",
      "  Games: 17 | Revenue (USD): $1,679,294,341\n",
      "  Success Rate: 100.0% | Avg Rating: 80.0%\n",
      "  Avg Revenue/Game: $98,782,020\n",
      "\n",
      "TOP 15 M&A TARGETS:\n",
      "================================================================================\n",
      "\n",
      "EroticGamesClub                          [nan]\n",
      "  M&A Score: 482.0 | Portfolio: 181 games\n",
      "  Revenue: $5,907,151 | Users: 1,810,000\n",
      "  Hit Rate: 0.0% | Retention: 0.0\n",
      "  Risk Score: 0.20\n",
      "\n",
      "Big Fish Games                           [nan]\n",
      "  M&A Score: 284.1 | Portfolio: 510 games\n",
      "  Revenue: $60,871,976 | Users: 6,280,000\n",
      "  Hit Rate: 2.5% | Retention: 0.0\n",
      "  Risk Score: 0.20\n",
      "\n",
      "Kagura Games                             [nan]\n",
      "  M&A Score: 244.1 | Portfolio: 153 games\n",
      "  Revenue: $105,487,608 | Users: 6,970,000\n",
      "  Hit Rate: 44.4% | Retention: 0.2\n",
      "  Risk Score: 0.21\n",
      "\n",
      "Pixel Games UK                           [nan]\n",
      "  M&A Score: 210.1 | Portfolio: 110 games\n",
      "  Revenue: $20,277,924 | Users: 3,870,000\n",
      "  Hit Rate: 0.9% | Retention: 0.0\n",
      "  Risk Score: 0.21\n",
      "\n",
      "Gamesforgames                            [nan]\n",
      "  M&A Score: 196.5 | Portfolio: 63 games\n",
      "  Revenue: $23,452,572 | Users: 845,000\n",
      "  Hit Rate: 1.6% | Retention: 0.0\n",
      "  Risk Score: 0.20\n",
      "\n",
      "I commissioned some series               [nan]\n",
      "  M&A Score: 195.6 | Portfolio: 57 games\n",
      "  Revenue: $2,306,794 | Users: 595,000\n",
      "  Hit Rate: 1.8% | Retention: 0.0\n",
      "  Risk Score: 0.21\n",
      "\n",
      "Do Games Limited                         [nan]\n",
      "  M&A Score: 191.4 | Portfolio: 52 games\n",
      "  Revenue: $10,756,447 | Users: 1,380,000\n",
      "  Hit Rate: 0.0% | Retention: 0.0\n",
      "  Risk Score: 0.21\n",
      "\n",
      "Archor Games                             [nan]\n",
      "  M&A Score: 184.8 | Portfolio: 70 games\n",
      "  Revenue: $1,089,327 | Users: 1,030,000\n",
      "  Hit Rate: 1.4% | Retention: 0.0\n",
      "  Risk Score: 0.21\n",
      "\n",
      "8floor                                   [nan]\n",
      "  M&A Score: 177.1 | Portfolio: 268 games\n",
      "  Revenue: $14,754,567 | Users: 2,820,000\n",
      "  Hit Rate: 0.4% | Retention: 0.0\n",
      "  Risk Score: 0.20\n",
      "\n",
      "indie.io                                 [nan]\n",
      "  M&A Score: 176.2 | Portfolio: 65 games\n",
      "  Revenue: $84,336,114 | Users: 3,975,000\n",
      "  Hit Rate: 35.4% | Retention: 0.1\n",
      "  Risk Score: 0.23\n",
      "\n",
      "Shiravune                                [nan]\n",
      "  M&A Score: 175.9 | Portfolio: 70 games\n",
      "  Revenue: $22,632,869 | Users: 1,095,000\n",
      "  Hit Rate: 50.0% | Retention: 0.0\n",
      "  Risk Score: 0.21\n",
      "\n",
      "EpiXR Games UG                           [nan]\n",
      "  M&A Score: 175.6 | Portfolio: 69 games\n",
      "  Revenue: $7,866,312 | Users: 740,000\n",
      "  Hit Rate: 10.1% | Retention: 0.0\n",
      "  Risk Score: 0.20\n",
      "\n",
      "Square Enix                              [nan]\n",
      "  M&A Score: 173.9 | Portfolio: 156 games\n",
      "  Revenue: $1,453,660,020 | Users: 72,420,000\n",
      "  Hit Rate: 43.6% | Retention: 0.6\n",
      "  Risk Score: 0.21\n",
      "\n",
      "HH-Games                                 [nan]\n",
      "  M&A Score: 172.0 | Portfolio: 193 games\n",
      "  Revenue: $15,118,055 | Users: 2,675,000\n",
      "  Hit Rate: 2.6% | Retention: 0.0\n",
      "  Risk Score: 0.20\n",
      "\n",
      "STuNT                                    [nan]\n",
      "  M&A Score: 170.3 | Portfolio: 46 games\n",
      "  Revenue: $862,498 | Users: 790,000\n",
      "  Hit Rate: 0.0% | Retention: 0.0\n",
      "  Risk Score: 0.22\n",
      "\n",
      "REORDERING COLUMNS...\n",
      "------------------------------------------------------------\n",
      "Columns reordered: 107 total\n",
      "   Organized columns: 76\n",
      "   Additional columns: 31\n",
      "\n",
      "SAVING CLEANED RESULTS\n",
      "------------------------------------------------------------\n",
      "Saved cleaned data: /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/steam_data_cleaned_complete.csv\n",
      "   Shape: (140082, 107)\n",
      "Saved publisher metrics: /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/publisher_metrics_cleaned.csv\n",
      "\n",
      "Creating sampled dataset...\n",
      "Saved sampled data (1000 records): /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/steam_data_cleaned_sample_1000.csv\n",
      "Saved top 100 publishers: /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/publisher_metrics_top100.csv\n",
      "Saved data quality report: /Users/timtoepper/Downloads/Banks_Assignment_Tim/output/data_quality_report.txt\n",
      "\n",
      "================================================================================\n",
      "ENHANCED DATA CLEANING PIPELINE COMPLETE!\n",
      "\n",
      "SUMMARY:\n",
      "  • Total games: 140,082\n",
      "  • Total columns: 107\n",
      "  • Publishers analyzed: 47,307\n",
      "  • Data quality: 84.6% complete\n",
      "  • Total revenue (USD): $229,581,004,175\n",
      "  • Average tags per game: 14.8\n",
      "\n",
      "Output files in /output folder:\n",
      "  • steam_data_cleaned_complete.csv (all data with USD conversion)\n",
      "  • publisher_metrics_cleaned.csv (USD-based metrics)\n",
      "  • data_quality_report.txt\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Complete Data Cleaning and Merging Pipeline - Enhanced Version\n",
    "# Handles all 8 data files with USD conversion and comprehensive cleaning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration - MODIFIED TO WORK AFTER SCRIPT 6\n",
    "DATA_DIR = Path(\"/Users/timtoepper/Downloads/Banks_Assignment_Tim\")\n",
    "\n",
    "# Check if Script 6 has been run\n",
    "sampled_outputs_dir = DATA_DIR / \"sampled_outputs\"\n",
    "if sampled_outputs_dir.exists() and (sampled_outputs_dir / \"games_cleaned.csv\").exists():\n",
    "    print(\"Found Script 6 outputs - using cleaned games data\")\n",
    "    GAMES_FILE = sampled_outputs_dir / \"games_cleaned.csv\"\n",
    "    USE_SAMPLED = False  # Set to True if you want to use the 10k sample instead\n",
    "    if USE_SAMPLED:\n",
    "        GAMES_FILE = sampled_outputs_dir / \"games_sampled.csv\"\n",
    "else:\n",
    "    print(\"Script 6 outputs not found - using original games.csv\")\n",
    "    GAMES_FILE = DATA_DIR / \"games.csv\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Games file: {GAMES_FILE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# CURRENCY CONVERSION SETUP\n",
    "# ============================================\n",
    "\n",
    "# Historical FX rates as of 2025-08-01 (fallback rates)\n",
    "FX_RATES_TO_USD = {\n",
    "    'USD': 1.0,\n",
    "    'EUR': 1.09,\n",
    "    'GBP': 1.28,\n",
    "    'JPY': 0.0067,\n",
    "    'CNY': 0.14,\n",
    "    'CAD': 0.72,\n",
    "    'AUD': 0.65,\n",
    "    'RUB': 0.011,\n",
    "    'BRL': 0.18,\n",
    "    'INR': 0.012,\n",
    "    'KRW': 0.00077,\n",
    "    'MXN': 0.055,\n",
    "    'SEK': 0.094,\n",
    "    'CHF': 1.16,\n",
    "    'PLN': 0.25,\n",
    "    'TRY': 0.030,\n",
    "    'ARS': 0.0011,\n",
    "    'NZD': 0.60,\n",
    "    'SGD': 0.74,\n",
    "    'HKD': 0.13,\n",
    "    'NOK': 0.093,\n",
    "    'DKK': 0.146,\n",
    "    'ZAR': 0.054,\n",
    "    'THB': 0.028,\n",
    "    'IDR': 0.000063,\n",
    "    'MYR': 0.22,\n",
    "    'PHP': 0.018,\n",
    "    'CZK': 0.044,\n",
    "    'HUF': 0.0028,\n",
    "    'ILS': 0.27,\n",
    "    'CLP': 0.0011,\n",
    "    'AED': 0.27,\n",
    "    'SAR': 0.27,\n",
    "    'RON': 0.22,\n",
    "    'COP': 0.00024,\n",
    "    'VND': 0.000040,\n",
    "    'UAH': 0.024,\n",
    "    'EGP': 0.021,\n",
    "    'PKR': 0.0036,\n",
    "    'BGN': 0.56,\n",
    "    'HRK': 0.14,\n",
    "    'LTL': 0.32,  # Historical\n",
    "    'LVL': 1.55,  # Historical\n",
    "    'EEK': 0.070, # Historical\n",
    "}\n",
    "\n",
    "def normalize_currency_code(currency):\n",
    "    r\"\"\"Normalize currency codes, handling \\N, 0, empty values\"\"\"\n",
    "    if pd.isna(currency) or currency == '' or currency == '\\\\N' or currency == 'N' or currency == '0' or currency == 0:\n",
    "        return 'USD'  # Default to USD\n",
    "    currency = str(currency).strip().upper()\n",
    "    if currency in FX_RATES_TO_USD:\n",
    "        return currency\n",
    "    return 'USD'  # Default for unknown currencies\n",
    "\n",
    "def convert_to_usd(amount, currency):\n",
    "    \"\"\"Convert any amount to USD using FX rates\"\"\"\n",
    "    if pd.isna(amount) or amount == 0:\n",
    "        return 0\n",
    "    currency = normalize_currency_code(currency)\n",
    "    rate = FX_RATES_TO_USD.get(currency, 1.0)\n",
    "    return amount * rate\n",
    "\n",
    "# ============================================\n",
    "# HELPER FUNCTIONS FOR DATA CLEANING\n",
    "# ============================================\n",
    "\n",
    "def clean_numeric_column(series):\n",
    "    r\"\"\"Clean numeric columns: handle \\N, convert to numeric\"\"\"\n",
    "    return pd.to_numeric(\n",
    "        series.replace('\\\\N', np.nan).replace('', np.nan),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "def clean_text_column(series):\n",
    "    r\"\"\"Clean text columns: handle \\N, strip whitespace\"\"\"\n",
    "    return series.replace('\\\\N', '').replace('', '').str.strip()\n",
    "\n",
    "def parse_owners_range(x):\n",
    "    r\"\"\"Parse Steam owner ranges like '10,000,000 .. 20,000,000'\"\"\"\n",
    "    if pd.isna(x) or x == '' or x == '\\\\N':\n",
    "        return 0\n",
    "    x = str(x).replace(',', '')\n",
    "    if '..' in x:\n",
    "        parts = x.split('..')\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                low = float(parts[0].strip())\n",
    "                high = float(parts[1].strip())\n",
    "                return (low + high) / 2  # Average of range\n",
    "            except:\n",
    "                return 0\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def clean_review_text(text):\n",
    "    \"\"\"Clean HTML tags and special characters from review text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    if text == '\\\\N' or text == 'N' or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    text = text.replace('&quot;', '\"')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    \n",
    "    # Remove escape characters\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    text = text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove <br /> and <br/> specifically\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_description_text(text):\n",
    "    \"\"\"Clean description text similar to review text cleaning\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    if text == '\\\\N' or text == 'N' or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    text = text.replace('&quot;', '\"')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&#39;', \"'\")\n",
    "    \n",
    "    # Remove escape characters\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    text = text.replace('\\\\', '')\n",
    "    \n",
    "    # Remove <br /> and <br/> specifically\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
    "    \n",
    "    # Clean up multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 1. LOAD AND CLEAN GAMES.CSV\n",
    "# ============================================\n",
    "print(\"\\n1. LOADING AND CLEANING GAMES.CSV\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "games_df = pd.read_csv(GAMES_FILE, low_memory=False)\n",
    "print(f\"Loaded: {len(games_df):,} games from {GAMES_FILE.name}\")\n",
    "\n",
    "# Note: If coming from Script 6, some cleaning may already be done\n",
    "if \"price_final\" in games_df.columns and \"price_final_cents\" not in games_df.columns:\n",
    "    print(\"  Note: Prices already expanded from JSON (Script 6 output)\")\n",
    "\n",
    "# Clean app_id\n",
    "games_df['app_id'] = clean_numeric_column(games_df['app_id'])\n",
    "games_df = games_df.dropna(subset=['app_id'])\n",
    "games_df['app_id'] = games_df['app_id'].astype(int)\n",
    "\n",
    "# Clean and normalize currency\n",
    "if 'currency' in games_df.columns:\n",
    "    games_df['currency'] = games_df['currency'].apply(normalize_currency_code)\n",
    "else:\n",
    "    games_df['currency'] = 'USD'\n",
    "\n",
    "# Clean prices\n",
    "for col in ['price_final_cents', 'price_initial_cents', 'discount_percent']:\n",
    "    if col in games_df.columns:\n",
    "        games_df[col] = clean_numeric_column(games_df[col])\n",
    "\n",
    "# Convert cents to dollars in original currency\n",
    "if 'price_final_cents' in games_df.columns:\n",
    "    games_df['price_final'] = games_df['price_final_cents'] / 100\n",
    "if 'price_initial_cents' in games_df.columns:\n",
    "    games_df['price_initial'] = games_df['price_initial_cents'] / 100\n",
    "\n",
    "# Convert to USD\n",
    "games_df['price_final_usd'] = games_df.apply(\n",
    "    lambda row: convert_to_usd(row.get('price_final', 0), row.get('currency', 'USD')), axis=1\n",
    ")\n",
    "games_df['price_initial_usd'] = games_df.apply(\n",
    "    lambda row: convert_to_usd(row.get('price_initial', 0), row.get('currency', 'USD')), axis=1\n",
    ")\n",
    "\n",
    "# Calculate average price for revenue calculations\n",
    "games_df['price_avg_usd'] = (games_df['price_final_usd'] + games_df['price_initial_usd']) / 2\n",
    "\n",
    "# Parse release date\n",
    "if 'release_date' in games_df.columns:\n",
    "    games_df['release_date'] = pd.to_datetime(games_df['release_date'], errors='coerce')\n",
    "\n",
    "# Clean is_free flag\n",
    "if 'is_free' in games_df.columns:\n",
    "    games_df['is_free'] = clean_numeric_column(games_df['is_free']).fillna(0).astype(bool)\n",
    "\n",
    "print(f\"Cleaned games: {len(games_df):,} rows × {len(games_df.columns)} columns\")\n",
    "\n",
    "# Initialize merged dataframe with ALL games columns\n",
    "merged = games_df.copy()\n",
    "\n",
    "# [REST OF THE SCRIPT CONTINUES EXACTLY AS IN THE DOCUMENT FROM LINE 251 ONWARDS]\n",
    "\n",
    "# ============================================\n",
    "# 2. LOAD AND CLEAN STEAMSPY_INSIGHTS.CSV\n",
    "# ============================================\n",
    "print(\"\\n2. LOADING AND CLEANING STEAMSPY_INSIGHTS.CSV\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Use csv.DictReader for malformed CSV\n",
    "    steamspy_data = []\n",
    "    with open(DATA_DIR / \"steamspy_insights.csv\", 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i % 25000 == 0 and i > 0:\n",
    "                print(f\"  Processing row {i:,}...\")\n",
    "            # Clean each row\n",
    "            clean_row = {}\n",
    "            for key, value in row.items():\n",
    "                if value == '\\\\N' or value == 'N' or value == '':\n",
    "                    clean_row[key] = None\n",
    "                else:\n",
    "                    clean_row[key] = value\n",
    "            steamspy_data.append(clean_row)\n",
    "    \n",
    "    steamspy_df = pd.DataFrame(steamspy_data)\n",
    "    print(f\"Loaded: {len(steamspy_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    steamspy_df['app_id'] = clean_numeric_column(steamspy_df['app_id'])\n",
    "    steamspy_df = steamspy_df.dropna(subset=['app_id'])\n",
    "    steamspy_df['app_id'] = steamspy_df['app_id'].astype(int)\n",
    "    \n",
    "    # Parse owners range (average of range)\n",
    "    if 'owners_range' in steamspy_df.columns:\n",
    "        steamspy_df['owners_avg'] = steamspy_df['owners_range'].apply(parse_owners_range)\n",
    "        print(f\"  Parsed owners - Average: {steamspy_df['owners_avg'].mean():,.0f}\")\n",
    "    \n",
    "    # Clean numeric columns\n",
    "    numeric_cols = ['concurrent_users_yesterday', 'playtime_average_forever', \n",
    "                   'playtime_average_2weeks', 'playtime_median_forever', \n",
    "                   'playtime_median_2weeks', 'price', 'initial_price', 'discount']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in steamspy_df.columns:\n",
    "            steamspy_df[col] = clean_numeric_column(steamspy_df[col])\n",
    "    \n",
    "    # Convert steamspy prices to USD\n",
    "    # Assuming steamspy prices are in cents\n",
    "    if 'price' in steamspy_df.columns:\n",
    "        if steamspy_df['price'].max() > 1000:  # Likely in cents\n",
    "            steamspy_df['price'] = steamspy_df['price'] / 100\n",
    "        steamspy_df['price_usd'] = steamspy_df['price']  # Assume USD for steamspy\n",
    "    \n",
    "    if 'initial_price' in steamspy_df.columns:\n",
    "        if steamspy_df['initial_price'].max() > 1000:\n",
    "            steamspy_df['initial_price'] = steamspy_df['initial_price'] / 100\n",
    "        steamspy_df['initial_price_usd'] = steamspy_df['initial_price']  # Assume USD\n",
    "    \n",
    "    # Clean text columns\n",
    "    for col in ['developer', 'publisher', 'languages', 'genres']:\n",
    "        if col in steamspy_df.columns:\n",
    "            steamspy_df[col] = clean_text_column(steamspy_df[col])\n",
    "    \n",
    "    # Merge with main dataset - keep ALL columns\n",
    "    steamspy_cols_to_merge = steamspy_df.columns.tolist()\n",
    "    merged = pd.merge(merged, steamspy_df[steamspy_cols_to_merge], \n",
    "                      on='app_id', how='left', suffixes=('', '_steamspy'))\n",
    "    \n",
    "    print(f\"Merged steamspy: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with steamspy: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 3. LOAD AND CLEAN REVIEWS.CSV\n",
    "# ============================================\n",
    "print(\"\\n3. LOADING AND CLEANING REVIEWS.CSV\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    reviews_df = pd.read_csv(DATA_DIR / \"reviews.csv\", \n",
    "                            on_bad_lines='skip',\n",
    "                            engine='python',\n",
    "                            na_values=['\\\\N', '\\\\\\\\N', 'N', 'NULL', ''],\n",
    "                            keep_default_na=True)\n",
    "    print(f\"Loaded: {len(reviews_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    reviews_df['app_id'] = clean_numeric_column(reviews_df['app_id'])\n",
    "    reviews_df = reviews_df.dropna(subset=['app_id'])\n",
    "    reviews_df['app_id'] = reviews_df['app_id'].astype(int)\n",
    "    \n",
    "    # Clean all numeric columns\n",
    "    numeric_cols = ['review_score', 'positive', 'negative', 'total', \n",
    "                   'metacritic_score', 'recommendations', 'steamspy_user_score', \n",
    "                   'steamspy_score_rank', 'steamspy_positive', 'steamspy_negative']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in reviews_df.columns:\n",
    "            reviews_df[col] = clean_numeric_column(reviews_df[col])\n",
    "    \n",
    "    # Clean review text\n",
    "    if 'reviews' in reviews_df.columns:\n",
    "        print(\"  Cleaning reviews text...\")\n",
    "        reviews_df['reviews'] = reviews_df['reviews'].fillna('')\n",
    "        reviews_df['reviews'] = reviews_df['reviews'].replace('\\\\N', '')\n",
    "        reviews_df['reviews_clean'] = reviews_df['reviews'].apply(clean_review_text)\n",
    "        valid_reviews = (reviews_df['reviews_clean'] != '').sum()\n",
    "        print(f\"  Cleaned reviews text: {valid_reviews:,} valid reviews\")\n",
    "    \n",
    "    if 'review_score_description' in reviews_df.columns:\n",
    "        reviews_df['review_score_description'] = clean_text_column(reviews_df['review_score_description'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if 'positive' in reviews_df.columns and 'negative' in reviews_df.columns:\n",
    "        reviews_df['total_reviews'] = reviews_df['positive'].fillna(0) + reviews_df['negative'].fillna(0)\n",
    "        reviews_df['positive_ratio'] = reviews_df['positive'] / (reviews_df['total_reviews'] + 1)\n",
    "        reviews_df['positive_ratio'] = reviews_df['positive_ratio'].fillna(0)\n",
    "    \n",
    "    # Merge with main dataset - keep ALL columns\n",
    "    reviews_cols_to_merge = reviews_df.columns.tolist()\n",
    "    merged = pd.merge(merged, reviews_df[reviews_cols_to_merge], \n",
    "                      on='app_id', how='left', suffixes=('', '_reviews'))\n",
    "    \n",
    "    print(f\"Merged reviews: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with reviews: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. LOAD AND CLEAN CATEGORIES.CSV\n",
    "# ============================================\n",
    "print(\"\\n4. LOADING AND CLEANING CATEGORIES.CSV\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    cat_df = pd.read_csv(DATA_DIR / \"categories.csv\", on_bad_lines='skip', engine='python')\n",
    "    print(f\"Loaded: {len(cat_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    cat_df['app_id'] = clean_numeric_column(cat_df['app_id'])\n",
    "    cat_df = cat_df.dropna(subset=['app_id'])\n",
    "    cat_df['app_id'] = cat_df['app_id'].astype(int)\n",
    "    \n",
    "    # Clean category names\n",
    "    if 'category' in cat_df.columns:\n",
    "        cat_df['category'] = clean_text_column(cat_df['category'])\n",
    "        cat_df = cat_df[cat_df['category'] != '']\n",
    "    \n",
    "    # Aggregate categories - keep ALL, dedupe and sort\n",
    "    cat_agg = cat_df.groupby('app_id')['category'].apply(\n",
    "        lambda x: '|'.join(sorted(set(x.dropna())))\n",
    "    ).reset_index()\n",
    "    cat_agg.columns = ['app_id', 'categories_all']\n",
    "    \n",
    "    # Extract features\n",
    "    cat_agg['category_count'] = cat_agg['categories_all'].apply(\n",
    "        lambda x: len(x.split('|')) if x else 0\n",
    "    )\n",
    "    cat_agg['has_multiplayer'] = cat_agg['categories_all'].str.contains('Multi-player|Multiplayer', case=False, na=False)\n",
    "    cat_agg['has_singleplayer'] = cat_agg['categories_all'].str.contains('Single-player|Singleplayer', case=False, na=False)\n",
    "    cat_agg['has_coop'] = cat_agg['categories_all'].str.contains('Co-op', case=False, na=False)\n",
    "    cat_agg['has_vr'] = cat_agg['categories_all'].str.contains('VR', case=False, na=False)\n",
    "    cat_agg['has_controller'] = cat_agg['categories_all'].str.contains('controller', case=False, na=False)\n",
    "    \n",
    "    # Merge\n",
    "    merged = pd.merge(merged, cat_agg, on='app_id', how='left')\n",
    "    print(f\"Merged categories: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with categories: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 5. LOAD AND CLEAN TAGS.CSV - ALL TAGS\n",
    "# ============================================\n",
    "print(\"\\n5. LOADING AND CLEANING TAGS.CSV (ALL TAGS)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    tags_df = pd.read_csv(DATA_DIR / \"tags.csv\", on_bad_lines='skip', engine='python')\n",
    "    print(f\"Loaded: {len(tags_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    tags_df['app_id'] = clean_numeric_column(tags_df['app_id'])\n",
    "    tags_df = tags_df.dropna(subset=['app_id'])\n",
    "    tags_df['app_id'] = tags_df['app_id'].astype(int)\n",
    "    \n",
    "    # Clean tag names\n",
    "    if 'tag' in tags_df.columns:\n",
    "        tags_df['tag'] = clean_text_column(tags_df['tag'])\n",
    "        tags_df = tags_df[tags_df['tag'] != '']\n",
    "    \n",
    "    # Get ALL tags per game, deduped and sorted alphabetically\n",
    "    tags_agg = tags_df.groupby('app_id')['tag'].apply(\n",
    "        lambda x: '|'.join(sorted(set(x.dropna())))\n",
    "    ).reset_index()\n",
    "    tags_agg.columns = ['app_id', 'tags_all']\n",
    "    \n",
    "    # Count unique tags\n",
    "    tags_agg['tag_count'] = tags_agg['tags_all'].apply(\n",
    "        lambda x: len(x.split('|')) if x else 0\n",
    "    )\n",
    "    \n",
    "    # Extract genre indicators from ALL tags\n",
    "    tags_agg['is_indie'] = tags_agg['tags_all'].str.contains('Indie', case=False, na=False)\n",
    "    tags_agg['is_action'] = tags_agg['tags_all'].str.contains('Action', case=False, na=False)\n",
    "    tags_agg['is_adventure'] = tags_agg['tags_all'].str.contains('Adventure', case=False, na=False)\n",
    "    tags_agg['is_rpg'] = tags_agg['tags_all'].str.contains('RPG', case=False, na=False)\n",
    "    tags_agg['is_strategy'] = tags_agg['tags_all'].str.contains('Strategy', case=False, na=False)\n",
    "    tags_agg['is_simulation'] = tags_agg['tags_all'].str.contains('Simulation', case=False, na=False)\n",
    "    tags_agg['is_puzzle'] = tags_agg['tags_all'].str.contains('Puzzle', case=False, na=False)\n",
    "    \n",
    "    # Merge\n",
    "    merged = pd.merge(merged, tags_agg, on='app_id', how='left')\n",
    "    print(f\"Merged tags: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    print(f\"  Average tags per game: {tags_agg['tag_count'].mean():.1f}\")\n",
    "    print(f\"  Max tags on a game: {tags_agg['tag_count'].max()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with tags: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 6. LOAD AND CLEAN GENRES.CSV\n",
    "# ============================================\n",
    "print(\"\\n6. LOADING AND CLEANING GENRES.CSV\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    genres_df = pd.read_csv(DATA_DIR / \"genres.csv\", on_bad_lines='skip', engine='python')\n",
    "    print(f\"Loaded: {len(genres_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    genres_df['app_id'] = clean_numeric_column(genres_df['app_id'])\n",
    "    genres_df = genres_df.dropna(subset=['app_id'])\n",
    "    genres_df['app_id'] = genres_df['app_id'].astype(int)\n",
    "    \n",
    "    # Clean genre names\n",
    "    if 'genre' in genres_df.columns:\n",
    "        genres_df['genre'] = clean_text_column(genres_df['genre'])\n",
    "        genres_df = genres_df[genres_df['genre'] != '']\n",
    "    \n",
    "    # Aggregate genres\n",
    "    genres_agg = genres_df.groupby('app_id')['genre'].apply(\n",
    "        lambda x: '|'.join(sorted(set(x.dropna())))\n",
    "    ).reset_index()\n",
    "    genres_agg.columns = ['app_id', 'genres_all']\n",
    "    \n",
    "    # Get primary genre\n",
    "    genres_agg['primary_genre'] = genres_agg['genres_all'].str.split('|').str[0]\n",
    "    genres_agg['genre_count'] = genres_agg['genres_all'].apply(\n",
    "        lambda x: len(x.split('|')) if x else 0\n",
    "    )\n",
    "    \n",
    "    # Merge\n",
    "    merged = pd.merge(merged, genres_agg, on='app_id', how='left')\n",
    "    print(f\"Merged genres: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with genres: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 7. LOAD AND CLEAN DESCRIPTIONS.CSV (FULL)\n",
    "# ============================================\n",
    "print(\"\\n7. LOADING AND CLEANING DESCRIPTIONS.CSV (FULL DATASET)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    desc_df = pd.read_csv(DATA_DIR / \"descriptions.csv\", \n",
    "                         on_bad_lines='skip', \n",
    "                         engine='python',\n",
    "                         na_values=['\\\\N', 'NULL'])\n",
    "    print(f\"Loaded: {len(desc_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    desc_df['app_id'] = clean_numeric_column(desc_df['app_id'])\n",
    "    desc_df = desc_df.dropna(subset=['app_id'])\n",
    "    desc_df['app_id'] = desc_df['app_id'].astype(int)\n",
    "    \n",
    "    # Clean description fields and create clean versions\n",
    "    description_fields = ['summary', 'extensive', 'about', 'short_description', 'detailed_description', 'about_the_game']\n",
    "    \n",
    "    for col in description_fields:\n",
    "        if col in desc_df.columns:\n",
    "            # Create cleaned version\n",
    "            desc_df[f'{col}_clean'] = desc_df[col].apply(clean_description_text)\n",
    "            # Add length of cleaned version\n",
    "            desc_df[f'{col}_length'] = desc_df[f'{col}_clean'].str.len().fillna(0)\n",
    "            # Drop the original dirty column\n",
    "            desc_df = desc_df.drop(columns=[col])\n",
    "            print(f\"  Cleaned {col}: {(desc_df[f'{col}_clean'] != '').sum():,} valid descriptions\")\n",
    "    \n",
    "    # Keep ALL description columns\n",
    "    desc_cols_to_merge = desc_df.columns.tolist()\n",
    "    merged = pd.merge(merged, desc_df[desc_cols_to_merge], \n",
    "                      on='app_id', how='left', suffixes=('', '_desc'))\n",
    "    \n",
    "    print(f\"Merged descriptions: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with descriptions: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 8. LOAD AND CLEAN PROMOTIONAL.CSV (FULL)\n",
    "# ============================================\n",
    "print(\"\\n8. LOADING AND CLEANING PROMOTIONAL.CSV (FULL DATASET)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    promo_df = pd.read_csv(DATA_DIR / \"promotional.csv\", \n",
    "                          on_bad_lines='skip', \n",
    "                          engine='python',\n",
    "                          na_values=['\\\\N', 'NULL'])\n",
    "    print(f\"Loaded: {len(promo_df):,} records\")\n",
    "    \n",
    "    # Clean app_id\n",
    "    promo_df['app_id'] = clean_numeric_column(promo_df['app_id'])\n",
    "    promo_df = promo_df.dropna(subset=['app_id'])\n",
    "    promo_df['app_id'] = promo_df['app_id'].astype(int)\n",
    "    \n",
    "    # Aggregate promotional materials per game\n",
    "    promo_count = promo_df.groupby('app_id').agg({\n",
    "        'app_id': 'count'  # Count promotional items\n",
    "    }).rename(columns={'app_id': 'promo_material_count'})\n",
    "    \n",
    "    # Get promotional content types if available\n",
    "    if 'type' in promo_df.columns:\n",
    "        promo_types = promo_df.groupby('app_id')['type'].apply(\n",
    "            lambda x: '|'.join(sorted(set(x.dropna().astype(str))))\n",
    "        ).reset_index()\n",
    "        promo_types.columns = ['app_id', 'promo_types']\n",
    "        promo_count = pd.merge(promo_count, promo_types, on='app_id', how='left')\n",
    "    \n",
    "    # Keep all promotional columns if they have other data\n",
    "    promo_cols_to_merge = promo_count.columns.tolist()\n",
    "    merged = pd.merge(merged, promo_count, on='app_id', how='left')\n",
    "    \n",
    "    print(f\"Merged promotional: {merged.shape[0]:,} rows × {merged.shape[1]} columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with promotional: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# 9. CALCULATE KPIs WITH USD\n",
    "# ============================================\n",
    "print(\"\\n9. CALCULATING KEY PERFORMANCE INDICATORS (USD-BASED)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Estimated revenue in USD\n",
    "if 'owners_avg' in merged.columns:\n",
    "    # Use average of initial and final USD prices\n",
    "    merged['estimated_revenue_usd'] = merged['owners_avg'] * merged['price_avg_usd']\n",
    "    merged['estimated_revenue_usd'] = merged['estimated_revenue_usd'].fillna(0)\n",
    "    total_revenue_usd = merged['estimated_revenue_usd'].sum()\n",
    "    print(f\"Estimated total revenue (USD): ${total_revenue_usd:,.0f}\")\n",
    "\n",
    "# Game age\n",
    "if 'release_date' in merged.columns:\n",
    "    merged['game_age_years'] = (pd.Timestamp.now() - merged['release_date']).dt.days / 365.25\n",
    "    merged['game_age_years'] = merged['game_age_years'].clip(lower=0).fillna(0)\n",
    "    print(f\"Average game age: {merged['game_age_years'].mean():.1f} years\")\n",
    "\n",
    "# Success metrics\n",
    "if 'total_reviews' in merged.columns:\n",
    "    threshold = merged['total_reviews'].quantile(0.75)\n",
    "    merged['is_successful'] = merged['total_reviews'] > threshold\n",
    "    print(f\"Successful games (>75th percentile): {merged['is_successful'].sum():,}\")\n",
    "\n",
    "if 'positive_ratio' in merged.columns:\n",
    "    merged['quality_score'] = merged['positive_ratio'] * 100\n",
    "    print(f\"Average quality score: {merged['quality_score'].mean():.1f}/100\")\n",
    "\n",
    "# Hit games (top 10% by ownership)\n",
    "if 'owners_avg' in merged.columns:\n",
    "    hit_threshold = merged['owners_avg'].quantile(0.90)\n",
    "    merged['is_hit'] = merged['owners_avg'] > hit_threshold\n",
    "    print(f\"Hit games (>90th percentile): {merged['is_hit'].sum():,}\")\n",
    "\n",
    "# ============================================\n",
    "# 9A. CLEAN PUBLISHER DATA\n",
    "# ============================================\n",
    "if 'publisher' in merged.columns:\n",
    "    # Clean publisher names\n",
    "    merged['publisher'] = merged['publisher'].fillna('Unknown')\n",
    "    merged['publisher'] = merged['publisher'].str.strip()\n",
    "    \n",
    "    # Filter out generic/bad publisher names\n",
    "    bad_publishers = ['Unknown', '', 'N/A', '\\\\N', 'None']\n",
    "    valid_publishers = ~merged['publisher'].isin(bad_publishers)\n",
    "    print(f\"Valid publishers: {valid_publishers.sum():,} games\")\n",
    "\n",
    "# ============================================\n",
    "# 9B. CALCULATE ADVANCED M&A KPIs\n",
    "# ============================================\n",
    "print(\"\\nCALCULATING ADVANCED M&A METRICS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1. Content Production Metrics - VECTORIZED\n",
    "print(\"Calculating Content Production Metrics...\")\n",
    "if 'release_date' in merged.columns and 'publisher' in merged.columns:\n",
    "    # Vectorized calculations - much faster!\n",
    "    merged['games_last_3y'] = merged.groupby('publisher')['game_age_years'].transform(\n",
    "        lambda x: (x <= 3).sum()\n",
    "    )\n",
    "    merged['release_cadence'] = merged['games_last_3y'] / 3\n",
    "    merged['recent_games_ratio'] = merged.groupby('publisher')['game_age_years'].transform(\n",
    "        lambda x: (x <= 2).sum() / len(x) if len(x) > 0 else 0\n",
    "    )\n",
    "\n",
    "# 2. Subscriber Retention Power\n",
    "print(\"Calculating Retention Metrics...\")\n",
    "if all(col in merged.columns for col in ['playtime_median_forever', 'playtime_average_forever', \n",
    "                                          'concurrent_users_yesterday', 'owners_avg']):\n",
    "    merged['retention_score'] = (\n",
    "        merged['playtime_median_forever'].fillna(0) * 0.3 +\n",
    "        merged['playtime_average_forever'].fillna(0) * 0.3 +\n",
    "        (merged['concurrent_users_yesterday'] / (merged['owners_avg'] + 1)) * 1000 * 0.4\n",
    "    )\n",
    "    \n",
    "    # Replay value relative to genre\n",
    "    if 'primary_genre' in merged.columns:\n",
    "        genre_avg_playtime = merged.groupby('primary_genre')['playtime_average_forever'].transform('median')\n",
    "        merged['replay_value'] = merged['playtime_average_forever'] / (genre_avg_playtime + 1)\n",
    "\n",
    "# 3. Franchise Detection\n",
    "print(\"Detecting Franchises...\")\n",
    "if 'name' in merged.columns:\n",
    "    # Simple franchise detection based on common patterns\n",
    "    merged['is_sequel'] = merged['name'].str.contains(\n",
    "        r'\\b[2-9]|\\bII|\\bIII|\\bIV|\\bV\\b|:|\\s-\\s', \n",
    "        regex=True, na=False\n",
    "    )\n",
    "    \n",
    "    # Calculate sequel performance if quality scores exist\n",
    "    if 'quality_score' in merged.columns:\n",
    "        sequel_quality = merged[merged['is_sequel']]['quality_score'].mean()\n",
    "        original_quality = merged[~merged['is_sequel']]['quality_score'].mean()\n",
    "        merged['sequel_performance_ratio'] = sequel_quality / (original_quality + 1)\n",
    "\n",
    "# 4. Audience Diversity Metrics\n",
    "print(\"Calculating Audience Metrics...\")\n",
    "if 'languages_clean' in merged.columns:\n",
    "    # Count languages (already have this, but calculate if missing)\n",
    "    if 'language_count' not in merged.columns:\n",
    "        merged['language_count'] = merged['languages_clean'].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # Language market score (languages × reach)\n",
    "    merged['language_market_score'] = merged['language_count'] * merged['owners_avg']\n",
    "\n",
    "# Content maturity diversity\n",
    "if 'tags_all' in merged.columns:\n",
    "    merged['has_mature_content'] = merged['tags_all'].str.contains(\n",
    "        'Mature|Adult|Gore|Violence|Sexual|Nudity', case=False, na=False\n",
    "    )\n",
    "    merged['has_family_content'] = merged['tags_all'].str.contains(\n",
    "        'Family|Kids|Casual|Educational|Cute', case=False, na=False\n",
    "    )\n",
    "\n",
    "# 5. Financial Efficiency Metrics\n",
    "print(\"Calculating Financial Efficiency...\")\n",
    "if all(col in merged.columns for col in ['category_count', 'language_count', \n",
    "                                          'has_multiplayer', 'tag_count']):\n",
    "    # FIX: Fill NaN values BEFORE converting to int\n",
    "    merged['has_multiplayer'] = merged['has_multiplayer'].fillna(False)\n",
    "    \n",
    "    # Development complexity proxy\n",
    "    merged['development_complexity'] = (\n",
    "        merged['category_count'].fillna(0) * 10 +\n",
    "        merged['language_count'].fillna(0) * 5 +\n",
    "        merged['has_multiplayer'].astype(int) * 50 +\n",
    "        merged['tag_count'].fillna(0) * 2\n",
    "    )\n",
    "    \n",
    "    merged['roi_efficiency'] = merged['estimated_revenue_usd'] / (merged['development_complexity'] + 1)\n",
    "    \n",
    "    # CAC efficiency\n",
    "    if 'promo_material_count' in merged.columns:\n",
    "        merged['cac_efficiency'] = merged['owners_avg'] / (merged['promo_material_count'].fillna(1) + 1)\n",
    "\n",
    "# 6. Risk Assessment Metrics\n",
    "print(\"Calculating Risk Metrics...\")\n",
    "\n",
    "# Platform dependency risk\n",
    "if 'tags_all' in merged.columns:\n",
    "    merged['is_pc_exclusive'] = ~merged['tags_all'].str.contains(\n",
    "        'Console|PlayStation|Xbox|Switch|Mobile', case=False, na=False\n",
    "    )\n",
    "    \n",
    "    # Technology risk\n",
    "    merged['uses_old_tech'] = merged['tags_all'].str.contains(\n",
    "        'Retro|Classic|Old School|8-bit|16-bit|Pixel', case=False, na=False\n",
    "    )\n",
    "    merged['uses_new_tech'] = merged['tags_all'].str.contains(\n",
    "        'VR|Ray Tracing|DLSS|Next-Gen|RTX|4K', case=False, na=False\n",
    "    )\n",
    "\n",
    "# Controversy risk from reviews\n",
    "if 'reviews_clean' in merged.columns:\n",
    "    merged['has_controversy'] = merged['reviews_clean'].str.contains(\n",
    "        'controversy|lawsuit|stolen|copyright|plagiar|scam|fraud|broken',\n",
    "        case=False, na=False\n",
    "    )\n",
    "\n",
    "# 7. Advanced Success Categories\n",
    "print(\"Calculating Advanced Success Metrics...\")\n",
    "\n",
    "# Commercial Success/Failure\n",
    "if 'estimated_revenue_usd' in merged.columns:\n",
    "    merged['is_commercial_success'] = merged['estimated_revenue_usd'] > 100000\n",
    "    merged['is_commercial_flop'] = (\n",
    "        (merged['estimated_revenue_usd'] < 5000) & \n",
    "        (merged['game_age_years'] > 0.25) &\n",
    "        (~merged['is_free'])\n",
    "    )\n",
    "\n",
    "# Critical Success/Failure\n",
    "if 'positive_ratio' in merged.columns and 'total_reviews' in merged.columns:\n",
    "    merged['is_critical_success'] = (\n",
    "        (merged['positive_ratio'] > 0.8) & \n",
    "        (merged['total_reviews'] >= 50)\n",
    "    )\n",
    "    merged['is_critical_flop'] = (\n",
    "        (merged['positive_ratio'] < 0.4) & \n",
    "        (merged['total_reviews'] >= 20)\n",
    "    )\n",
    "\n",
    "# Combined categories\n",
    "merged['is_commercial_success'] = merged['is_commercial_success'].fillna(False)\n",
    "merged['is_critical_success'] = merged['is_critical_success'].fillna(False)\n",
    "merged['is_commercial_flop'] = merged['is_commercial_flop'].fillna(False)\n",
    "merged['is_critical_flop'] = merged['is_critical_flop'].fillna(False)\n",
    "\n",
    "merged['is_total_success'] = merged['is_commercial_success'] & merged['is_critical_success']\n",
    "merged['is_total_flop'] = merged['is_commercial_flop'] & merged['is_critical_flop']\n",
    "merged['is_hidden_gem'] = merged['is_critical_success'] & merged['is_commercial_flop']\n",
    "\n",
    "# Dead game detection\n",
    "if 'concurrent_users_yesterday' in merged.columns:\n",
    "    merged['is_currently_dead'] = (\n",
    "        (merged['concurrent_users_yesterday'] == 0) & \n",
    "        (merged['game_age_years'] > 0.5)\n",
    "    )\n",
    "    merged['is_zombie_game'] = (\n",
    "        (merged['concurrent_users_yesterday'] == 0) &\n",
    "        (merged['owners_avg'] < 1000) &\n",
    "        (merged['total_reviews'] < 10) &\n",
    "        (merged['game_age_years'] > 0.5)\n",
    "    )\n",
    "\n",
    "print(f\"Commercial successes: {merged['is_commercial_success'].sum():,}\")\n",
    "print(f\"Critical successes: {merged['is_critical_success'].sum():,}\")\n",
    "print(f\"Hidden gems: {merged['is_hidden_gem'].sum():,}\")\n",
    "print(f\"Dead games: {merged['is_currently_dead'].sum() if 'is_currently_dead' in merged.columns else 0:,}\")\n",
    "\n",
    "# ============================================\n",
    "# 10. ENHANCED PUBLISHER AGGREGATION & ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\nCALCULATING PUBLISHER METRICS FOR M&A\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Define output directory\n",
    "output_dir = DATA_DIR / \"output\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if 'publisher' in merged.columns and valid_publishers.sum() > 0:\n",
    "    # Basic publisher metrics\n",
    "    publisher_metrics = merged[valid_publishers].groupby('publisher').agg({\n",
    "        'app_id': 'count',\n",
    "        'owners_avg': lambda x: x.fillna(0).sum(),\n",
    "        'estimated_revenue_usd': lambda x: x.fillna(0).sum(),\n",
    "        'positive_ratio': lambda x: x[x > 0].mean() if len(x[x > 0]) > 0 else 0,\n",
    "        'total_reviews': lambda x: x.fillna(0).sum(),\n",
    "        'has_multiplayer': lambda x: x.sum() if 'has_multiplayer' in merged.columns else 0,\n",
    "        'has_vr': lambda x: x.sum() if 'has_vr' in merged.columns else 0,\n",
    "        'is_successful': lambda x: x.sum() if 'is_successful' in merged.columns else 0,\n",
    "        'tag_count': lambda x: x.mean() if 'tag_count' in merged.columns else 0,\n",
    "    }).round(2)\n",
    "    \n",
    "    publisher_metrics.columns = ['game_count', 'total_players', 'total_revenue_usd', \n",
    "                                 'avg_rating', 'total_reviews', 'multiplayer_games', \n",
    "                                 'vr_games', 'successful_games', 'avg_tags_per_game']\n",
    "    \n",
    "    # Add calculated metrics\n",
    "    publisher_metrics['success_rate'] = (publisher_metrics['successful_games'] / publisher_metrics['game_count'] * 100).round(1)\n",
    "    publisher_metrics['avg_revenue_per_game_usd'] = (publisher_metrics['total_revenue_usd'] / publisher_metrics['game_count']).round(0)\n",
    "    \n",
    "    # Sort by USD revenue\n",
    "    publisher_metrics = publisher_metrics.sort_values('total_revenue_usd', ascending=False)\n",
    "    \n",
    "    # Save basic publisher metrics\n",
    "    publisher_file = output_dir / \"publisher_metrics_cleaned.csv\"\n",
    "    publisher_metrics.to_csv(publisher_file)\n",
    "    print(f\"Saved publisher metrics: {publisher_file}\")\n",
    "    \n",
    "    # Enhanced M&A-specific aggregation\n",
    "    publisher_ma_metrics = merged[valid_publishers].groupby('publisher').agg({\n",
    "        'app_id': 'count',\n",
    "        'owners_avg': lambda x: x.fillna(0).sum(),\n",
    "        'estimated_revenue_usd': lambda x: x.fillna(0).sum(),\n",
    "        'release_cadence': 'first' if 'release_cadence' in merged.columns else lambda x: 0,\n",
    "        'recent_games_ratio': 'first' if 'recent_games_ratio' in merged.columns else lambda x: 0,\n",
    "        'retention_score': 'mean' if 'retention_score' in merged.columns else lambda x: 0,\n",
    "        'replay_value': 'mean' if 'replay_value' in merged.columns else lambda x: 0,\n",
    "        'concurrent_users_yesterday': 'sum',\n",
    "        'is_commercial_success': 'sum' if 'is_commercial_success' in merged.columns else lambda x: 0,\n",
    "        'is_critical_success': 'sum' if 'is_critical_success' in merged.columns else lambda x: 0,\n",
    "        'is_hidden_gem': 'sum' if 'is_hidden_gem' in merged.columns else lambda x: 0,\n",
    "        'is_total_flop': 'sum' if 'is_total_flop' in merged.columns else lambda x: 0,\n",
    "        'is_zombie_game': 'sum' if 'is_zombie_game' in merged.columns else lambda x: 0,\n",
    "        'roi_efficiency': 'mean' if 'roi_efficiency' in merged.columns else lambda x: 0,\n",
    "        'cac_efficiency': 'mean' if 'cac_efficiency' in merged.columns else lambda x: 0,\n",
    "        'is_sequel': 'sum' if 'is_sequel' in merged.columns else lambda x: 0,\n",
    "        'is_pc_exclusive': 'mean' if 'is_pc_exclusive' in merged.columns else lambda x: 0,\n",
    "        'uses_new_tech': 'mean' if 'uses_new_tech' in merged.columns else lambda x: 0,\n",
    "        'uses_old_tech': 'mean' if 'uses_old_tech' in merged.columns else lambda x: 0,\n",
    "        'has_controversy': 'sum' if 'has_controversy' in merged.columns else lambda x: 0,\n",
    "        'has_mature_content': 'mean' if 'has_mature_content' in merged.columns else lambda x: 0,\n",
    "        'has_family_content': 'mean' if 'has_family_content' in merged.columns else lambda x: 0,\n",
    "        'language_market_score': 'sum' if 'language_market_score' in merged.columns else lambda x: 0,\n",
    "    }).round(3)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    publisher_ma_metrics.rename(columns={\n",
    "        'app_id': 'portfolio_size',\n",
    "        'owners_avg': 'total_user_base',\n",
    "        'estimated_revenue_usd': 'total_revenue_usd',\n",
    "        'is_commercial_success': 'commercial_success_count',\n",
    "        'is_critical_success': 'critical_success_count',\n",
    "        'is_hidden_gem': 'hidden_gem_count',\n",
    "        'is_total_flop': 'flop_count',\n",
    "        'is_zombie_game': 'zombie_count',\n",
    "        'is_sequel': 'sequel_count',\n",
    "        'is_pc_exclusive': 'pc_exclusive_ratio',\n",
    "        'uses_new_tech': 'new_tech_ratio',\n",
    "        'uses_old_tech': 'old_tech_ratio',\n",
    "        'has_controversy': 'controversy_count',\n",
    "        'has_mature_content': 'mature_content_ratio',\n",
    "        'has_family_content': 'family_content_ratio'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    publisher_ma_metrics['hit_rate'] = (\n",
    "        publisher_ma_metrics['critical_success_count'] / \n",
    "        (publisher_ma_metrics['portfolio_size'] + 0.001)\n",
    "    )\n",
    "    publisher_ma_metrics['commercial_success_rate'] = (\n",
    "        publisher_ma_metrics['commercial_success_count'] / \n",
    "        (publisher_ma_metrics['portfolio_size'] + 0.001)\n",
    "    )\n",
    "    publisher_ma_metrics['zombie_rate'] = (\n",
    "        publisher_ma_metrics['zombie_count'] / \n",
    "        (publisher_ma_metrics['portfolio_size'] + 0.001)\n",
    "    )\n",
    "    publisher_ma_metrics['franchise_ratio'] = (\n",
    "        publisher_ma_metrics['sequel_count'] / \n",
    "        (publisher_ma_metrics['portfolio_size'] + 0.001)\n",
    "    )\n",
    "    \n",
    "    # Revenue concentration (Herfindahl Index)\n",
    "    revenue_concentration = merged[valid_publishers].groupby('publisher').apply(\n",
    "        lambda x: ((x['estimated_revenue_usd'] / (x['estimated_revenue_usd'].sum() + 0.001)) ** 2).sum()\n",
    "    )\n",
    "    publisher_ma_metrics['revenue_concentration'] = revenue_concentration\n",
    "    \n",
    "    # Calculate M&A scores with safe division\n",
    "    max_portfolio = max(publisher_ma_metrics['portfolio_size'].max(), 1)\n",
    "    max_roi = max(publisher_ma_metrics['roi_efficiency'].max(), 0.001)\n",
    "    max_retention = max(publisher_ma_metrics['retention_score'].max(), 0.001)\n",
    "    max_language = max(publisher_ma_metrics['language_market_score'].max(), 1)\n",
    "    \n",
    "    publisher_ma_metrics['content_score'] = (\n",
    "        publisher_ma_metrics['release_cadence'].fillna(0) * 0.4 +\n",
    "        publisher_ma_metrics['recent_games_ratio'].fillna(0) * 0.3 +\n",
    "        (publisher_ma_metrics['portfolio_size'] / max_portfolio) * 0.3\n",
    "    ) * 100\n",
    "    \n",
    "    publisher_ma_metrics['quality_score'] = (\n",
    "        publisher_ma_metrics['hit_rate'].fillna(0) * 0.5 +\n",
    "        publisher_ma_metrics['commercial_success_rate'].fillna(0) * 0.3 +\n",
    "        (1 - publisher_ma_metrics['zombie_rate'].fillna(0)) * 0.2\n",
    "    ) * 100\n",
    "    \n",
    "    # Safe revenue per user calculation\n",
    "    revenue_per_user = publisher_ma_metrics['total_revenue_usd'] / (publisher_ma_metrics['total_user_base'] + 1)\n",
    "    max_revenue_per_user = max(revenue_per_user.max(), 1)\n",
    "    \n",
    "    publisher_ma_metrics['efficiency_score'] = (\n",
    "        (publisher_ma_metrics['roi_efficiency'] / max_roi) * 0.5 +\n",
    "        (revenue_per_user / max_revenue_per_user) * 0.5\n",
    "    ) * 100\n",
    "    \n",
    "    publisher_ma_metrics['strategic_fit_score'] = (\n",
    "        publisher_ma_metrics['retention_score'].fillna(0) / max_retention * 0.4 +\n",
    "        publisher_ma_metrics['franchise_ratio'].fillna(0) * 0.3 +\n",
    "        publisher_ma_metrics['language_market_score'].fillna(0) / max_language * 0.3\n",
    "    ) * 100\n",
    "    \n",
    "    # Risk adjustment\n",
    "    publisher_ma_metrics['risk_score'] = (\n",
    "        publisher_ma_metrics['revenue_concentration'].fillna(0) * 0.3 +\n",
    "        publisher_ma_metrics['pc_exclusive_ratio'].fillna(0) * 0.2 +\n",
    "        (publisher_ma_metrics['controversy_count'] / (publisher_ma_metrics['portfolio_size'] + 0.001)).fillna(0) * 0.3 +\n",
    "        publisher_ma_metrics['zombie_rate'].fillna(0) * 0.2\n",
    "    )\n",
    "    \n",
    "    # Final M&A score\n",
    "    publisher_ma_metrics['ma_score'] = (\n",
    "        publisher_ma_metrics['content_score'] * 0.25 +\n",
    "        publisher_ma_metrics['quality_score'] * 0.25 +\n",
    "        publisher_ma_metrics['efficiency_score'] * 0.20 +\n",
    "        publisher_ma_metrics['strategic_fit_score'] * 0.20 +\n",
    "        (1 - publisher_ma_metrics['risk_score']) * 100 * 0.10\n",
    "    )\n",
    "    \n",
    "    # Assign tiers\n",
    "    publisher_ma_metrics['acquisition_tier'] = pd.cut(\n",
    "        publisher_ma_metrics['ma_score'],\n",
    "        bins=[0, 30, 50, 70, 100],\n",
    "        labels=['Avoid', 'Monitor', 'Strategic', 'Must Have']\n",
    "    )\n",
    "    \n",
    "    # Sort by M&A score\n",
    "    publisher_ma_metrics = publisher_ma_metrics.sort_values('ma_score', ascending=False)\n",
    "    \n",
    "    # Save enhanced publisher metrics\n",
    "    ma_metrics_file = output_dir / \"publisher_ma_analysis.csv\"\n",
    "    publisher_ma_metrics.to_csv(ma_metrics_file)\n",
    "    print(f\"Saved M&A analysis: {ma_metrics_file}\")\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nTOP 20 PUBLISHERS BY REVENUE (USD):\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, row in publisher_metrics.head(20).iterrows():\n",
    "        print(f\"\\n{str(idx)[:50]:<50}\")\n",
    "        print(f\"  Games: {row['game_count']:.0f} | Revenue (USD): ${row['total_revenue_usd']:,.0f}\")\n",
    "        print(f\"  Success Rate: {row['success_rate']:.1f}% | Avg Rating: {row['avg_rating']*100:.1f}%\")\n",
    "        print(f\"  Avg Revenue/Game: ${row['avg_revenue_per_game_usd']:,.0f}\")\n",
    "    \n",
    "    print(\"\\nTOP 15 M&A TARGETS:\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, row in publisher_ma_metrics.head(15).iterrows():\n",
    "        print(f\"\\n{str(idx)[:40]:<40} [{row['acquisition_tier']}]\")\n",
    "        print(f\"  M&A Score: {row['ma_score']:.1f} | Portfolio: {row['portfolio_size']:.0f} games\")\n",
    "        print(f\"  Revenue: ${row['total_revenue_usd']:,.0f} | Users: {row['total_user_base']:,.0f}\")\n",
    "        print(f\"  Hit Rate: {row['hit_rate']:.1%} | Retention: {row['retention_score']:.1f}\")\n",
    "        print(f\"  Risk Score: {row['risk_score']:.2f}\")\n",
    "# ============================================\n",
    "# 11. REORDER COLUMNS FOR BETTER ORGANIZATION\n",
    "# ============================================\n",
    "print(\"\\nREORDERING COLUMNS...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Define column order groups\n",
    "column_order = [\n",
    "    # Core identifiers\n",
    "    'app_id',\n",
    "    'name',\n",
    "    'type',\n",
    "    \n",
    "    # Basic info\n",
    "    'release_date',\n",
    "    'game_age_years',\n",
    "    'is_free',\n",
    "    \n",
    "    # Publishers and developers\n",
    "    'developer',\n",
    "    'publisher',\n",
    "    \n",
    "    # Pricing - original currency\n",
    "    'currency',\n",
    "    'price_final_cents',\n",
    "    'price_initial_cents',\n",
    "    'price_final',\n",
    "    'price_initial',\n",
    "    'price_final_formatted',\n",
    "    'discount_percent',\n",
    "    'discount',\n",
    "    \n",
    "    # Pricing - USD converted\n",
    "    'price_final_usd',\n",
    "    'price_initial_usd',\n",
    "    'price_avg_usd',\n",
    "    'price_usd',\n",
    "    'initial_price_usd',\n",
    "    \n",
    "    # User base and engagement\n",
    "    'owners_range',\n",
    "    'owners_avg',\n",
    "    'concurrent_users_yesterday',\n",
    "    'playtime_average_forever',\n",
    "    'playtime_average_2weeks',\n",
    "    'playtime_median_forever',\n",
    "    'playtime_median_2weeks',\n",
    "    \n",
    "    # Revenue and success metrics\n",
    "    'estimated_revenue_usd',\n",
    "    'is_successful',\n",
    "    'is_hit',\n",
    "    \n",
    "    # Reviews and ratings\n",
    "    'review_score',\n",
    "    'review_score_description',\n",
    "    'positive',\n",
    "    'negative',\n",
    "    'total',\n",
    "    'total_reviews',\n",
    "    'positive_ratio',\n",
    "    'quality_score',\n",
    "    'metacritic_score',\n",
    "    'recommendations',\n",
    "    'steamspy_user_score',\n",
    "    'steamspy_score_rank',\n",
    "    'steamspy_positive',\n",
    "    'steamspy_negative',\n",
    "    \n",
    "    # Review text (cleaned versions)\n",
    "    'reviews_clean',\n",
    "    \n",
    "    # Languages\n",
    "    'languages',\n",
    "    'languages_clean',\n",
    "    'languages_full_audio',\n",
    "    \n",
    "    # Categories\n",
    "    'categories_all',\n",
    "    'category_count',\n",
    "    'has_multiplayer',\n",
    "    'has_singleplayer',\n",
    "    'has_coop',\n",
    "    'has_vr',\n",
    "    'has_controller',\n",
    "    \n",
    "    # Tags\n",
    "    'tags_all',\n",
    "    'tag_count',\n",
    "    'is_indie',\n",
    "    'is_action',\n",
    "    'is_adventure',\n",
    "    'is_rpg',\n",
    "    'is_strategy',\n",
    "    'is_simulation',\n",
    "    'is_puzzle',\n",
    "    \n",
    "    # Genres\n",
    "    'genres',\n",
    "    'genres_all',\n",
    "    'primary_genre',\n",
    "    'genre_count',\n",
    "    \n",
    "    # Descriptions (cleaned versions)\n",
    "    'summary_clean',\n",
    "    'summary_length',\n",
    "    'extensive_clean',\n",
    "    'extensive_length',\n",
    "    'about_clean',\n",
    "    'about_length',\n",
    "    'short_description_clean',\n",
    "    'short_description_length',\n",
    "    'detailed_description_clean',\n",
    "    'detailed_description_length',\n",
    "    'about_the_game_clean',\n",
    "    'about_the_game_length',\n",
    "    \n",
    "    # Promotional\n",
    "    'promo_material_count',\n",
    "    'promo_types',\n",
    "]\n",
    "\n",
    "# Get existing columns\n",
    "existing_cols = [col for col in column_order if col in merged.columns]\n",
    "\n",
    "# Get any remaining columns not in the order list\n",
    "remaining_cols = [col for col in merged.columns if col not in existing_cols]\n",
    "\n",
    "# Combine ordered and remaining columns\n",
    "final_column_order = existing_cols + remaining_cols\n",
    "\n",
    "# Reorder the dataframe\n",
    "merged = merged[final_column_order]\n",
    "print(f\"Columns reordered: {len(final_column_order)} total\")\n",
    "print(f\"   Organized columns: {len(existing_cols)}\")\n",
    "print(f\"   Additional columns: {len(remaining_cols)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 12. SAVE CLEANED DATA\n",
    "# ============================================\n",
    "print(\"\\nSAVING CLEANED RESULTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "output_dir = DATA_DIR / \"output\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save main dataset\n",
    "merged_file = output_dir / \"steam_data_cleaned_complete.csv\"\n",
    "merged.to_csv(merged_file, index=False)\n",
    "print(f\"Saved cleaned data: {merged_file}\")\n",
    "print(f\"   Shape: {merged.shape}\")\n",
    "\n",
    "# Save publisher metrics\n",
    "if 'publisher_metrics' in locals():\n",
    "    publisher_file = output_dir / \"publisher_metrics_cleaned.csv\"\n",
    "    publisher_metrics.to_csv(publisher_file)\n",
    "    print(f\"Saved publisher metrics: {publisher_file}\")\n",
    "\n",
    "# Save a sampled version (1000 records) of the final dataset\n",
    "print(\"\\nCreating sampled dataset...\")\n",
    "sampled_merged = merged.sample(n=min(1000, len(merged)), random_state=42)\n",
    "sampled_file = output_dir / \"steam_data_cleaned_sample_1000.csv\"\n",
    "sampled_merged.to_csv(sampled_file, index=False)\n",
    "print(f\"Saved sampled data (1000 records): {sampled_file}\")\n",
    "\n",
    "# Also create a sampled publisher metrics (top 100)\n",
    "if 'publisher_metrics' in locals():\n",
    "    sampled_publisher_file = output_dir / \"publisher_metrics_top100.csv\"\n",
    "    publisher_metrics.head(100).to_csv(sampled_publisher_file)\n",
    "    print(f\"Saved top 100 publishers: {sampled_publisher_file}\")\n",
    "\n",
    "# Save enhanced data quality report\n",
    "quality_report = output_dir / \"data_quality_report.txt\"\n",
    "with open(quality_report, 'w') as f:\n",
    "    f.write(\"DATA QUALITY REPORT - ENHANCED VERSION\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Dataset Shape:\\n\")\n",
    "    f.write(f\"  Rows: {len(merged):,}\\n\")\n",
    "    f.write(f\"  Columns: {len(merged.columns)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Currency Conversion:\\n\")\n",
    "    if 'currency' in merged.columns:\n",
    "        currency_counts = merged['currency'].value_counts().head(10)\n",
    "        for curr, count in currency_counts.items():\n",
    "            f.write(f\"  {curr}: {count:,} games\\n\")\n",
    "    f.write(f\"  Total Revenue (USD): ${merged['estimated_revenue_usd'].sum():,.0f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Top 20 Columns with Missing Data:\\n\")\n",
    "    missing = merged.isnull().sum()\n",
    "    missing_pct = (missing / len(merged) * 100).round(2)\n",
    "    missing_sorted = missing.sort_values(ascending=False).head(20)\n",
    "    for col in missing_sorted.index:\n",
    "        # Handle potential None values in column name, missing count, and percentage\n",
    "        col_name = str(col) if col is not None else \"UNKNOWN\"\n",
    "        miss_count = int(missing[col]) if pd.notna(missing[col]) else 0\n",
    "        pct_value = float(missing_pct[col]) if pd.notna(missing_pct[col]) else 0.0\n",
    "        f.write(f\"  {col_name:<35} {miss_count:>7,} ({pct_value:>5.1f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\n\\nData Completeness: {(merged.notna().sum().sum() / (len(merged) * len(merged.columns)) * 100):.1f}%\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\nAll Column Names ({} total):\\n\".format(len(merged.columns)))\n",
    "    for i, col in enumerate(merged.columns, 1):\n",
    "        col_name = str(col) if col is not None else \"UNKNOWN\"\n",
    "        f.write(f\"  {i:3}. {col_name}\\n\")\n",
    "print(f\"Saved data quality report: {quality_report}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENHANCED DATA CLEANING PIPELINE COMPLETE!\")\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"  • Total games: {len(merged):,}\")\n",
    "print(f\"  • Total columns: {len(merged.columns)}\")\n",
    "print(f\"  • Publishers analyzed: {publisher_metrics.shape[0] if 'publisher_metrics' in locals() else 0:,}\")\n",
    "print(f\"  • Data quality: {(merged.notna().sum().sum() / (len(merged) * len(merged.columns)) * 100):.1f}% complete\")\n",
    "print(f\"  • Total revenue (USD): ${merged['estimated_revenue_usd'].sum():,.0f}\")\n",
    "print(f\"  • Average tags per game: {merged['tag_count'].mean():.1f}\" if 'tag_count' in merged.columns else \"\")\n",
    "print(\"\\nOutput files in /output folder:\")\n",
    "print(\"  • steam_data_cleaned_complete.csv (all data with USD conversion)\")\n",
    "print(\"  • publisher_metrics_cleaned.csv (USD-based metrics)\")\n",
    "print(\"  • data_quality_report.txt\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db276f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8be672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
